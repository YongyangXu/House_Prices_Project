# House_Prices_Project
Ask a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this project’s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.  
This dataset contains 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, that can help us to predict the most correct house prices.   
I finally did a Lasso regressions that performs best with a cross validation RMSE-score of 0.1311. Given the fact that there is a lot of multicolinearity among the variables, this was expected. Lasso does not select a substantial number of the available variables in its model, as it is supposed to do. Moreover, the XGBoost model also performs very well with a cross validation RMSE of 0.1319!  
Deep into the project details, I did many approaches to understand the dataset and clean it. First I checked the data size and structure, including the shape of of some important factors. I did a concentration filter analysis to exclude useless factors. Then I tried to imputing missing data, label encoding, and factorizing variables. I imputed data differently as their meaning is different. Using mode for the majority of numerical data,0 for some ordinary data, and None for non-ordinary data which will be hot coded later. After all factors have no NAs, character variables are converted into either numeric labels of into factors.  
Then, I checked the Correlations to see the multicolinearity problem and ran a quick random forest to find the variable importance. I dropped the variables with low variable importance when they have high correlated factors. I tried to do some feature engineering like combing 4 bathroom variables together with different coefficients, which are not very important individually, but it turned out to be non-effective to the final model. So I decided to leave it.   
Finally, in the modelling part, I preprocessed predictor variables, solved skewness of the response variable and indenpendent variables with skewness larger than 0.8 by taking log, and normalized of the numeric predictors. For Lasso model, I used caret cross validation to find the best value for lambda, which is the only hyperparameter that needs to be tuned for the lasso model. And Lasso dealt with multicolinearity well by not using about 45% of the available variables in the model. For Xgboost model, I used grid search to find the best hyperparameter tuning, and it gave the best Max_depth, learning rate and Min_child_weight. I used the best tuning to do 5-fold cross validation to determine the best number of rounds.  
